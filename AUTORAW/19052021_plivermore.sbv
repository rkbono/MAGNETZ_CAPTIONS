0:00:01.680,0:00:08.480
so hi everybody um thank you for uh joining this 
week's uh magnet seminar um it's good to see

0:00:08.480,0:00:13.440
another great turnout of people we've got over 50 
people uh already i'm sure there'll be a few more

0:00:13.440,0:00:20.160
uh jumping in uh last minute so for those who 
um aren't too familiar with the magnet seminars

0:00:20.160,0:00:28.000
we have a 25 to 30 minute presentation so we just 
kindly ask that you keep your microphones mediated

0:00:28.000,0:00:34.480
during the presentation if you do have any um 
connection issues and turning off your your video

0:00:34.480,0:00:40.720
feed can also help to to improve um a connection 
um at the end of the seminar we'll have chance for

0:00:40.720,0:00:48.240
a 10-15 minute um chit chat and discussion so we 
welcome questions um either um um via video and

0:00:48.240,0:00:53.520
microphone but if you don't want to ask a question 
yourself please just type it into the chat and um

0:00:53.520,0:00:58.880
either myself or or one of the other conveners 
will read it out for you and as always most of

0:00:58.880,0:01:03.440
us are working at home in various forms of home 
offices so if you do have to go halfway through

0:01:03.440,0:01:09.440
the seminars i just go it's not a problem there'll 
be a recording made available on youtube uh later

0:01:09.440,0:01:14.160
on and at the very end of the seminar we have a 
chance for a bit of social catch-up with everybody

0:01:14.160,0:01:17.920
um and none of that's recorded so it's a 
bit of a free-for-all just to chit chat

0:01:17.920,0:01:23.440
and catch up with everybody um and so today 
i'm really pleased to say that we have phil

0:01:23.440,0:01:28.800
livermore from uh university of leeds i just love 
the road from us in liverpool and he's going to

0:01:28.800,0:01:35.440
be talking about uh rq intensity spikes in the 
levant region so i will hand over to you phil

0:01:38.320,0:01:42.160
thank you and a very good 
morning to you from leeds

0:01:50.400,0:01:52.160
can i just check that you can all see that

0:01:54.000,0:02:03.840
yeah great okay yes uh so uh i want to tell you 
uh today about some some work that i've been uh

0:02:03.840,0:02:08.960
undertaking recently with my collaborators on 
this eve gallery and alex fournier in paris

0:02:10.240,0:02:14.880
which is focused on understanding 
archaeomagnetic intensity variations

0:02:14.880,0:02:22.000
in particular the proposed geomagnetic spikes in 
the in the level and an alternative title to this

0:02:22.000,0:02:28.400
talk is really counting spikes in noisy data 
which really gets to the heart of what i'm i'm

0:02:28.400,0:02:35.600
going to tell you about so the the story of 
the geomagnetic spike starts about started

0:02:35.600,0:02:43.200
about a decade ago with the publication of of 
several papers uh where the authors looked at at

0:02:43.200,0:02:51.360
several archemagnetic sites in the levant region 
which includes modern day israel and jordan and

0:02:52.400,0:02:59.040
they they looked at slag deposits 
which are waste deposits from iron age

0:02:59.760,0:03:06.640
copper smelting sites and in particular these 
sites were very well stratified so the oldest

0:03:06.640,0:03:11.520
stuff at the top the most recent uh sorry the 
oldest stuff at the bottom the most recent

0:03:11.520,0:03:18.560
at the top everything's very well layered uh and 
because of this the authors were able to propose

0:03:18.560,0:03:24.960
a very tight sequential chronology uh based 
on this on this a well-defined stratification

0:03:26.160,0:03:34.480
um and the authors analyzed various slag 
and pottery samples for paleo intensity and

0:03:34.480,0:03:42.880
they also dated some of the layers using 
carbon-14 analysis and what they found was

0:03:42.880,0:03:50.800
quite staggering and that was two two 
proposed uh spikes in the geomagnetic field

0:03:51.360,0:04:01.280
uh one about 980 bc and one about 890. 
uh so set up on a background of fairly uh

0:04:02.480,0:04:09.120
not quite sedate but fairly fairly low level 
second variation these two spikes showed massive

0:04:09.120,0:04:15.520
intensity increases uh of about a factor of 
a hundred percent only over a few decades

0:04:18.400,0:04:24.960
and in the intervening 10 years since since these 
studies first came out uh whilst of course the

0:04:24.960,0:04:31.920
data set has uh has improved it's increased 
in number has been updated been refined by

0:04:32.480,0:04:38.960
improved criteria for data selection nevertheless 
there remains strong evidence of rapid change

0:04:41.120,0:04:48.160
um and the the essence of the talk 
really is about how we construct a

0:04:48.960,0:04:56.320
a curve through the uh the the data set 
um which is an expanded version of what

0:04:56.320,0:05:01.200
you see in front of you um and how we use 
that to understand your magnetic spikes

0:05:03.200,0:05:08.720
so geomagnetic spikes occurred during what's 
termed the levantine iron age anomaly which

0:05:08.720,0:05:21.440
was a a a a temporal high in the intensity uh here 
you can see several global magnetic field models

0:05:22.960,0:05:28.560
plotted uh in amongst the individual 
data uh localized to this region

0:05:30.800,0:05:37.040
so why is it important to understand these spikes 
well crucially there's no historical or modern day

0:05:37.040,0:05:46.320
analog to to these these are the only uh these are 
the only known occurrence of such rapid magnetic

0:05:46.320,0:05:52.640
field chains that we have and indeed if you look 
at the typical rate of change of intensity uh

0:05:53.920,0:05:59.360
0.75 to about two and a half micro testers a 
year this is somewhere between five and twenty

0:05:59.360,0:06:05.040
times the present day maximum rate of change 
of intensity so there's definitely something

0:06:05.040,0:06:10.800
here to explain and understand spikes 
are really difficult to reproduce in

0:06:10.800,0:06:15.760
in geodynamo models so there's there's 
a gap in our in our understanding

0:06:19.120,0:06:24.240
to to make the magnetic or to to explain 
magnetic field change we either need to

0:06:24.240,0:06:30.880
rely on one of one or both of two processes one 
is advection of magnetic field and stretching

0:06:30.880,0:06:35.840
within earth's liquid core and the other is 
diffusion of magnetic field from within the core

0:06:37.920,0:06:40.880
but it's true it it's it's the case that

0:06:43.920,0:06:50.560
such rapid intensity changes definitely challenges 
our understanding of of of these processes so

0:06:50.560,0:06:56.240
either we need anonymously fast flows or 
we need some form of abnormal emergence

0:06:56.240,0:07:01.840
of magnetic field from within the 
core that can explain these features

0:07:03.920,0:07:12.000
so in order to um analyze this we we collected 
all published archaeomagnetic data in the region

0:07:12.000,0:07:21.360
of 1200 to 500 bc which which roughly uh defines 
the periods of geomagnetic spikes and these were

0:07:21.360,0:07:26.160
all reduced to palmeira in syria which was 
roughly at the center point of the dataset

0:07:28.480,0:07:35.120
the the published archaeomagnetic data are 
heterogeneous by which i mean they they either

0:07:35.120,0:07:41.760
refer to fragment level or thermal unit level 
samples so for example a shard a brick or a kiln

0:07:42.480,0:07:48.720
or they refer to a group level average over some 
number of fragments which are assumed to be of the

0:07:48.720,0:07:57.840
same age uh so in in the figure you can see the 
the data set that we collected from the literature

0:07:59.440,0:08:10.160
and you can also see a global model which is the 
sureq iron age model showing the general trend of

0:08:11.440,0:08:15.360
showing showing the the general trend 
according to that model uh in the region

0:08:15.360,0:08:21.840
and you can see the discrepancy of the fragment 
of of some of these uh of these samples um

0:08:22.880,0:08:26.320
extending way beyond the two 
sigma error bounds of the model

0:08:28.480,0:08:34.720
most of the data i showed is fragment level 
data and so for most of this presentation

0:08:34.720,0:08:39.840
i'm going to focus on this data set and 
here we see all the fragment level samples

0:08:41.440,0:08:45.280
as a plotted with intensity and age

0:08:48.080,0:08:53.280
and and the the key question is what is the 
intensity curve the variation of the intensity

0:08:53.280,0:08:58.960
with time that's consistent with this data set 
and from that curve we can then assess the number

0:08:58.960,0:09:04.720
and the nature of of the geomagnetic spikes and 
it's important that we have such a curve because

0:09:04.720,0:09:10.080
it's very hard otherwise to objectively assess 
whether or not we've had rapid intensity change

0:09:11.200,0:09:19.520
as an example of that this is these are the data 
points around 980 bc where one of the spikes were

0:09:19.520,0:09:27.120
proposed which seems to fit well with the data 
set as shown in the in the figure here's here's

0:09:27.120,0:09:31.440
another of the spikes which are proposed 
okay you might think that sound that that

0:09:31.440,0:09:36.320
looks uh all very convincing but the more 
you look of course the more spikes you see

0:09:37.040,0:09:42.240
so here's here's some more data which look uh 
look like i might define a spike and here's

0:09:42.240,0:09:47.200
some others and you can keep going and you see 
the more you look you see spikes everywhere

0:09:49.280,0:09:53.680
so this is there's there probably aren't 
so many spikes in this data set but we

0:09:53.680,0:09:59.120
need some way of up to objectively 
assess uh what what what's going on

0:10:02.080,0:10:09.760
uh i mentioned the fragment data set uh but if 
we look uh only at the data defined by averages

0:10:09.760,0:10:12.800
uh of of n data points that 
seem to be of the same age

0:10:13.360,0:10:20.160
then then of course th there are much fewer 
uh members of this data set and here they are

0:10:21.040,0:10:24.400
and i'll come back to these uh 
towards the end of the presentation

0:10:26.960,0:10:35.200
so the key question is how do we fit a model 
to these data the data are are non-uniform

0:10:35.200,0:10:39.680
in time so there are some times where 
there are more uh more data than others

0:10:41.280,0:10:47.680
the data are uncertain intensity and the 
uncertainties are different for each data point

0:10:48.480,0:10:56.800
the samples all have uncertainty in age and 
some of the samples are age ordered specifically

0:10:56.800,0:11:02.720
those derived from the stratified data sets i i 
introduced right at the start of the presentation

0:11:05.120,0:11:09.440
on the on the modeling side what kind of what 
kind of properties and model might we want

0:11:10.080,0:11:16.880
well ideally we want a model that is as simple as 
possible and you could say has a minimal curvature

0:11:16.880,0:11:22.880
that is required by the data we don't want to over 
fit the data we want a simple model as possible

0:11:23.760,0:11:32.000
we'd also like a model that can handle rapid 
variations at some times but steady variations

0:11:32.000,0:11:36.960
at other times so we'd like some kind of modeling 
that can accommodate multiple time scales

0:11:38.880,0:11:43.520
and we'd also of course like to have some 
handle on the uncertainty of any any model fit

0:11:46.160,0:11:52.880
so fitting fitting data or fitting model fits 
through data is a very common practice in in

0:11:52.880,0:12:00.720
the geomagnetism community um and typically when 
people construct job global geomagnetic models

0:12:01.520,0:12:04.560
rather than local models like 
i'm going to talk about here

0:12:05.360,0:12:12.800
typically what people do is they introduce a 
model and they then they smooth it using some

0:12:12.800,0:12:20.480
some form of of global penalization of complexity 
the disadvantage with that is it doesn't really

0:12:20.480,0:12:24.960
allow for multiple time scales you typically 
have a similar smoothing of the model everywhere

0:12:26.720,0:12:30.480
what i'm going to talk today 
about is a new approach which is a

0:12:31.120,0:12:37.840
based on bayesian modeling which doesn't have any 
explicit damping where the data self select times

0:12:38.800,0:12:45.200
where rapid change is needed and this 
modeling process provides formal uncertainties

0:12:47.200,0:12:53.440
so bayesian methods are named after thomas bayes 
who was an 18th century statistician after whom a

0:12:53.440,0:13:01.680
very famous famous bayes formula that you see in 
front of you was named this formula relates the

0:13:01.680,0:13:05.680
what's term the posterior probability 
which is the probability of a model

0:13:05.680,0:13:14.400
that's the thing we'd like given the data set d to 
various things so the posterior probability is the

0:13:14.400,0:13:18.480
product of something called the likelihood here 
which which is the probability of the data given

0:13:18.480,0:13:24.640
a model times a prior divided by a normalization 
factor here which is the called the evidence

0:13:26.000,0:13:33.200
so what we'd like to find is the posterior 
distribution of some time dependent models

0:13:33.200,0:13:39.840
given the data set now we can only really do this 
numerically and so what we do is we create an

0:13:39.840,0:13:47.600
ensemble uh using something termed a monte carlo 
markup chain process mcmc i describe that in it

0:13:48.800,0:13:54.960
i'll describe that briefly uh and when we create 
the ensemble the models with higher probability

0:13:54.960,0:14:00.800
are sampled more often and we need some way 
to characterize our model ensemble so the way

0:14:00.800,0:14:04.240
i'm going to describe this is by using the median

0:14:04.800,0:14:08.080
median bottle however of course we 
have the full statistics of the model

0:14:10.880,0:14:18.800
so in terms of the model what do i mean well 
the model is a piecewise linear fit through

0:14:19.520,0:14:23.840
through the data and a 
whole bunch of unknowns here

0:14:24.480,0:14:31.200
the number of the interior vertices so the number 
of linear segments of this of this linear fit

0:14:31.920,0:14:36.640
is unknown as are the position of the 
interior vertices so in other words where the

0:14:37.280,0:14:41.200
the points at which the linear 
segments to uh connect with each other

0:14:42.800,0:14:50.400
uh in our in our modeling process we also 
allow the data ages to be found by the model

0:14:50.400,0:14:57.840
so the data ranges themselves are unknown and 
we explore the model space using a random walk

0:14:58.880,0:15:05.200
any bayesian procedure needs to 
define prior information which is the

0:15:06.240,0:15:13.040
information it's a it's a formulation of 
our beliefs on the probability prior to the

0:15:14.080,0:15:21.120
introduction of any data and so we need to propose 
some prior distributions on the number of vertices

0:15:21.840,0:15:29.520
and the age and intensity of each internal vertex 
and we also need to impose a distribution of

0:15:29.520,0:15:35.920
on each data mage and all of these prior 
distributions are are reasonable um and i'll

0:15:35.920,0:15:41.440
point you to the reference where you can look up 
exactly what we've done at the end we also need to

0:15:42.800,0:15:48.560
we also need to have a or decide upon a likelihood 
which we assume to be normally distributed

0:15:50.480,0:15:56.080
the method i'm going to talk about i'm describing 
is is transdimensional which is a fancy word for

0:15:56.080,0:16:00.960
really saying that the number of model parameters 
is also an unknown so in other words the number

0:16:00.960,0:16:07.040
of linear segments is not is not determined 
by by the user it's determined by the data

0:16:07.040,0:16:14.960
themselves and there's no explicit regularization 
there's no there's no dumping applied instead

0:16:14.960,0:16:20.800
we rely upon the natural uh the natural 
nature of bayesian methods to be parsonal

0:16:20.800,0:16:27.520
parsimonious that is to to favor low 
dimensional models uh above complex models

0:16:29.760,0:16:32.800
each model is a is a piecewise linear

0:16:34.000,0:16:39.120
fit but when you average them you get a 
smooth evolution as i'll demonstrate shortly

0:16:40.800,0:16:47.200
okay so this modeling process uh we've 
named age hyper parameter reverse jump

0:16:47.200,0:16:52.320
monte carlo markov chain it's a very long title 
and i'll just break it down for you a little bit

0:16:52.880,0:16:57.920
so age hyper parameter means that the data 
ages are part of the model so as part of

0:16:57.920,0:17:04.560
the statistics that we generate we all we we not 
only derive the intensity evolution but we also

0:17:05.200,0:17:14.640
uh calculate the um the full statistics of all the 
data ages as well reverse jump just means that the

0:17:14.640,0:17:20.880
algorithm algorithm can reverse up and down the 
dimensionality of the system so that is we can

0:17:20.880,0:17:26.800
we can look for both simple and complex models and 
traverse between them and then this bit at the end

0:17:26.800,0:17:31.840
the monte carlo markup chain basically means it's 
an intelligent random walk through model space

0:17:33.760,0:17:40.080
so i just wanted to describe to you a few 
a few results that accompanied the paper

0:17:40.080,0:17:46.640
that described this method uh a few years ago now 
before we get on to the levantine uh dataset this

0:17:46.640,0:17:55.200
data set i'm showing you here we term paris 700. 
it's a it's a data set of archaea intensity uh

0:17:55.200,0:18:03.440
samples all found within 700 kilometers of paris 
you can see here plotted intensity against time

0:18:04.080,0:18:08.240
and we we're going to i'm going to show you what 
happens when we apply this method through through

0:18:08.240,0:18:15.600
these data so this is what happens you can see the 
the data set in front of you with the error bars

0:18:16.400,0:18:26.000
and i've characterized this distribution of models 
which is the posterior distribution by the average

0:18:26.000,0:18:31.520
the median and the mode in different colors 
and they pretty much over overlap each other

0:18:33.120,0:18:38.880
and you can see a measure of 
uncertainty of this bayesian uh modeling

0:18:39.840,0:18:47.760
by the 95 percent credible intervals so 
there's this there's a there's a few things

0:18:48.400,0:18:55.840
that are of note here firstly where the data 
require complicated time dependence then the model

0:18:56.640,0:19:02.080
accommodates that by being by showing a very 
strong time dependence for example you can see

0:19:03.520,0:19:11.040
there's very well resolved oscillations in 
intensity however where the data do not require

0:19:11.920,0:19:18.160
complex time dependence uh none is none is 
returned and and so you can see this very

0:19:18.720,0:19:23.840
simple constant fit through the intensity 
because the data are simply happy

0:19:25.120,0:19:31.440
for that that is a consistent model and does 
they don't require any additional complexity

0:19:35.120,0:19:41.680
and if you're interested you can find 
this this modeling uh it's on github

0:19:42.560,0:19:49.120
i'm not going to give you a live demonstration 
of it now but but the the model is actually

0:19:49.120,0:19:55.280
coded in fortran for speed and all you need 
to do is if it is go to the github repository

0:19:55.280,0:20:03.360
download it compile it uh configure the input file 
and then run the code and a typical ensemble size

0:20:03.360,0:20:09.760
of about 5 million ensemble members takes about a 
minute on a laptop so it's very quick and you'll

0:20:09.760,0:20:14.640
also find the data sets and the python scripts 
that generate all the figures i'll show today

0:20:16.320,0:20:21.680
so what happens when we apply this modeling 
procedure to the levantine data set

0:20:22.720,0:20:30.160
well in this figure i showed you i'm showing you 
the fragment level data set with the error bars

0:20:32.000,0:20:38.560
on intense intensity and age and 
in purple you can see the median of

0:20:38.560,0:20:42.720
this uh on of this bayesian model fit to the data

0:20:45.360,0:20:52.480
and in the panel beneath beneath that 
in in red you can see the rate of change

0:20:53.040,0:21:00.560
of of this median curve as a function of time 
so as i said i i've i've showed you the original

0:21:01.200,0:21:07.840
fragment level data set to accompany this figure 
but as i mentioned earlier the metal so the the

0:21:07.840,0:21:16.800
method also gives you statistics on the sample age 
themselves and and so the method actually gives a

0:21:16.800,0:21:23.200
preferred age and intensity uh for each sample 
okay and i'm going to show you what that looks

0:21:23.200,0:21:31.520
like now and so you can see that the the samples 
move closer to the curve of course because they're

0:21:32.320,0:21:38.640
everything's trying to be consistent there are 
locate there are some times where the samples

0:21:38.640,0:21:44.560
don't apparently seem to match the curve but of 
course these samples are pulled uh to the left

0:21:44.560,0:21:52.480
and to the right so because i'm plotting the 
median age as derived by this modeling process

0:21:52.480,0:21:57.840
the the ages lie in between the two branches 
of the curve that's why they fall in the middle

0:22:01.280,0:22:07.760
okay so having found a a way of 
fitting a curve through the data

0:22:07.760,0:22:12.400
set we'd like we're now likely trying 
to characterize the geomagnetic spikes

0:22:15.040,0:22:18.080
and in order to do that we needed 
to write down what a spike was

0:22:18.800,0:22:23.040
and so we proposed a number of 
characteristics so the first of which

0:22:23.040,0:22:29.120
is that a geomagnetic spike represents a peak 
in intensity and in particular exceeds the

0:22:29.120,0:22:35.520
neighboring minimum by five microtesla so in other 
words it's a well pronounced peak in intensity

0:22:37.760,0:22:44.000
the second criteria is on the rate 
of change and in particular a spike

0:22:45.520,0:22:50.560
must have a high rate of change and at some 
point it exceeds the present day rate of change

0:22:50.560,0:22:58.240
which is about 0.12 testers a year and the 
times at which this happens then defines the

0:22:58.240,0:23:04.560
width of the spike so we have some way of defining 
when the spike starts and when and when it ends

0:23:06.240,0:23:10.800
and the spike also needs to 
represent anomalously large

0:23:10.800,0:23:16.320
intensity variation which we take to 
mean greater than 0.6 microteslas a year

0:23:18.960,0:23:24.880
and these definitions build upon other 
other proposals in the literature um

0:23:25.680,0:23:30.400
one thing that's worth mentioning is that our 
dash of the the definition of the spikes here

0:23:30.400,0:23:35.840
has no defined time scale so spikes could be 10 
years long they could also be a hundred years long

0:23:38.880,0:23:46.160
so what happens when we apply our definition 
of spikes then to the intensity variation curve

0:23:46.160,0:23:51.200
well we actually find six geomagnetic 
spikes that are consistent with the data

0:23:52.000,0:23:56.560
according to the fragment 
level uh data set and i've

0:23:59.040,0:24:05.760
i've drawn the duration of the spike uh 
within within the blue boxes so six blue boxes

0:24:05.760,0:24:11.760
uh each defining a spike and each has a star 
with a different color uh and so you can see

0:24:12.400,0:24:17.840
uh clearly where the where the center 
points are the spikes are and their age

0:24:18.960,0:24:26.320
so importantly two of these spikes have already 
been proposed in the literature but there are four

0:24:26.320,0:24:34.640
others which are new at least new in the sense 
that they've not been spotted before but of course

0:24:34.640,0:24:39.760
this is all based upon existing data so these are 
new spikes but discovered in the existing data set

0:24:41.920,0:24:49.360
and what can we say about the duration of 
spikes well if we if we chop out the spikes

0:24:49.360,0:24:56.080
from the uh intensity variation curve and we 
plot them on a centered age figure like this

0:24:56.080,0:25:00.640
so they're all stacked one on top of the other of 
course they're all the six spikes all in different

0:25:00.640,0:25:09.200
colors we can we can very easily see uh how how 
long the the spikes have have lasted and they vary

0:25:09.200,0:25:15.600
from the shortest spike of about 30 years which is 
the yellow one uh right up to 100 years which is

0:25:15.600,0:25:20.320
the purple one and they're generally symmetric 
in age but they're not necessarily for example

0:25:20.320,0:25:27.840
the blue the blue spike uh in the middle here 
you can see is a little bit skewed to one side

0:25:31.120,0:25:38.560
uh i lastly want to say something very briefly 
about the sensitivity of the of this conclusion

0:25:38.560,0:25:49.600
and of the geomagnetic spikes um so not 93 of the 
139 data fragment level data contain very small

0:25:49.600,0:25:56.320
intensity errors which might be underestimates 
of the of their true intensity error

0:25:56.960,0:26:01.200
and so what we can do is we can test 
the sensitivity of this by sequentially

0:26:01.840,0:26:09.840
increasing the minimum intensity error from zero 
uh up to three four five and six microteslas

0:26:11.360,0:26:16.240
so by increasing the error budget of course 
what we might expect is that we can fit

0:26:16.240,0:26:23.840
another smoother curve through the data and 
thereby the number of spikes would go down

0:26:24.480,0:26:30.960
so this is what we find if we impose a minimum 
of three micro teslas intensity error uh

0:26:30.960,0:26:38.240
the six spikes reduced to five spikes but not 
by an awful lot there used to be a spike here

0:26:38.240,0:26:44.480
at about 790 bc which only marginally doesn't 
fulfill our criterion for being a spike

0:26:45.360,0:26:50.400
okay so it's it's all it's almost six spikes 
so this this minimum of three microtesters

0:26:50.400,0:26:55.760
doesn't make an awful lot of difference to 
our conclusions but as you start to increase

0:26:55.760,0:27:05.840
the intensity error budget our six spikes now 
drop to four when we reach a five microtesla

0:27:06.480,0:27:15.520
threshold we we drop down now to one spike 
and this is an interesting uh value for the

0:27:15.520,0:27:20.880
uncertainty in intensity because it's a level 
of uncertainty considered by other authors

0:27:23.360,0:27:28.320
i'm also showing here in green the shawq iron 
age model that is shown right at the start

0:27:28.320,0:27:32.080
which is beginning to look 
quite similar to this smoother

0:27:32.080,0:27:39.360
intensity evolution and as we go up to six 
microteslas uh we're now dropping we've now

0:27:39.360,0:27:47.840
dropped to zero spikes and the evolution now 
very closely follows this shorku iron age model

0:27:51.040,0:27:54.880
i mentioned right at the start that there 
was not only a fragment level data set for

0:27:54.880,0:28:03.520
the levantine region but the group level data set 
which comprised averages over at least n fragments

0:28:03.520,0:28:11.440
assume dated at the same age these were the data 
sets and what happens when we apply our analysis

0:28:11.440,0:28:21.360
to these well we don't see many spikes at all for 
the for the group level uh data set consisting of

0:28:21.360,0:28:29.200
averages over at least two samples we find only 
one spike um and if we increase the stringency

0:28:29.200,0:28:37.280
of the averaging to at least three fragments 
we find no spikes these these data points are

0:28:38.480,0:28:45.360
are the posterior median intensity and 
age so they're the the corresponding

0:28:45.360,0:28:50.960
data which is most consistent um 
with with the intensity evolution

0:28:53.920,0:28:55.680
okay so it's going to wrap up here

0:28:57.680,0:29:02.400
so what i've talked about here is the area 
of gym magnetic spikes and i think it gives

0:29:02.400,0:29:08.880
a really unique opportunity to study very rapid 
intensity change uh i've briefly introduced

0:29:09.520,0:29:16.880
a bayesian method that's ideally suited to 
characterizing rapid changes in intensity and

0:29:16.880,0:29:23.520
taken at face value the fragment level data set 
appears to require six spikes which is a lot more

0:29:23.520,0:29:30.880
than than has been proposed so far in the 
literature um and if you if you reassess

0:29:30.880,0:29:36.880
the data either by looking looking at the group 
averaging or by increasing the error budget you

0:29:36.880,0:29:45.840
get a smoother variation and fewer spikes so the 
real take home message here is that the levantine

0:29:45.840,0:29:53.840
dataset may require more complex behavior than 
previously thought during this this era of spikes

0:29:54.720,0:30:02.800
and this raises challenging questions for core 
modelers like myself um to try and explain uh what

0:30:03.520,0:30:09.920
what was going on during that time um 
right at the bottom here is a reference

0:30:10.560,0:30:16.960
uh which for a paper which recently came out 
describing uh what i presented today um with

0:30:16.960,0:30:22.960
a lot more detail and there's a reference here 
right at the bottom for the code that i introduced

0:30:22.960,0:30:30.000
earlier that you that you can freely download and 
run and that's it thank you for your attention

0:30:33.040,0:30:37.120
excellent thank you very much for that that was 
a really interesting talk um we can all give

0:30:37.120,0:30:45.280
phil a a virtual uh round of applause uh through 
zoom um and i'm sure there's gonna be an absolute

0:30:45.280,0:30:50.640
ton of questions because that was a really uh 
uh personally find that really exciting talk so

0:30:50.640,0:30:58.960
um i will open the floor to questions and i can 
see that uh andy begin as the first uh with his

0:30:58.960,0:31:09.040
hand hi phil hi andy nice talk um yeah i just 
wanted um have you looked at relative errors

0:31:09.040,0:31:13.520
at all rather than applying an absolute 
error allowing to scale with the actual

0:31:13.520,0:31:17.840
paleo intensity itself because that might 
be a more realistic way of looking at it

0:31:18.880,0:31:24.400
uh that is a good question and we did think 
very carefully about what errors to use

0:31:25.520,0:31:32.160
um in the paper i i didn't really want to go 
too much into this in the talk uh just to keep

0:31:32.160,0:31:40.240
it snappy um but in the paper we do discuss 
this at some length and we we tried using

0:31:40.880,0:31:50.800
uh what some of the authors um uh of of 
the levantine data uh called an extended

0:31:51.440,0:32:02.080
uh intensity error um so we we we tried to look at 
reasonable choices for yes for the intensity error

0:32:02.080,0:32:08.320
um and um well you can read about it in the 
paper but basically it doesn't really change

0:32:08.320,0:32:16.720
our conclusion all right thanks thanks andy so 
um kathy constable is the next with her hand up

0:32:17.360,0:32:24.960
yeah thanks uh i enjoyed your talk presentation 
phil uh i just wanted to um raise a comment

0:32:24.960,0:32:32.960
you mentioned that uh chris and i in our papers 
in 2017 2018 had difficulty finding spikes and

0:32:32.960,0:32:38.480
numerical dynamo models and i think that's 
not a completely accurate characterization of

0:32:39.040,0:32:44.080
um what we what we said in those papers we did 
actually find quite a number of spikes in dynamo

0:32:44.080,0:32:50.560
models whether they would meet the criteria that 
you specify here um we haven't explicitly tested

0:32:51.520,0:32:57.040
but we did actually look for rapid changes in the 
field and found quite a number in terms of the

0:32:57.040,0:33:02.640
field strength so i'd say it was still a fairly 
open question as to whether we find these things

0:33:02.640,0:33:07.040
in the numerical simulations and there is 
of course some difficulty in deciding what

0:33:07.040,0:33:14.720
the time scale they're occurring on would be yes 
thank you that's a good point cathy um in one of

0:33:14.720,0:33:20.000
your studies with chris you also looked at the 
localization of the spikes which is which is

0:33:20.560,0:33:27.200
a broad question that i've not really touched 
upon here and as to whether whether the spikes are

0:33:28.240,0:33:33.120
seen only in one place or in a broader region and 
i think that's that's another interesting question

0:33:34.400,0:33:38.800
yeah i think that uh inevitably if they're 
coming from the core then they have to occur

0:33:38.800,0:33:44.160
over the broader region uh that's certainly 
an interesting question for the helium

0:33:44.160,0:33:49.200
that the paleomagnetic data ought to be able 
to address as well as the numerical simulations

0:33:49.200,0:33:53.440
yeah and the numerical simulations we 
saw that the biggest changes in intensity

0:33:53.440,0:33:59.360
seem to occur at higher generally but at mid to 
high latitudes rather than at low likelihoods

0:34:01.760,0:34:08.960
so you're kind of in between yes 
yes okay thanks very much kathy um

0:34:09.520,0:34:16.240
we'll move on to gunther well first let me 
thank you phil it was really good to talk

0:34:16.240,0:34:23.600
and i like it because it was very calm and 
there was a assurance given and well thank you

0:34:24.240,0:34:30.800
it was very nice to listen to and i have a 
question uh i mean you talk about spikes uh

0:34:30.800,0:34:34.640
why don't you talk about dips because it 
looks like dips should work the same way

0:34:36.560,0:34:40.320
yes you're quite right maybe that we 
should have inverse spikes or something yes

0:34:41.200,0:34:43.520
that's right so uh let me find a picture

0:34:46.160,0:34:53.520
so yes so you're quite right um i i guess 
it's we're really following our lead from

0:34:53.520,0:34:59.280
the original papers that spotted this 
very rapid intensity uh intensity change

0:35:00.560,0:35:07.840
um but you are but you are quite right in 
that uh the the the concerns about getting

0:35:08.480,0:35:14.080
intensity increases also pertain to increasing 
decreases as well because it's the same

0:35:14.080,0:35:18.160
physical process that you need to 
explain uh both of both of these

0:35:22.240,0:35:29.840
we haven't we haven't actually looked at at uh 
dips as you as you as you put it only the spikes

0:35:30.560,0:35:36.640
thank you cheers cheers um lisa 
i have your hand up now i have a

0:35:37.680,0:35:39.920
thank you phil that was a 
great talk i really liked it

0:35:42.320,0:35:47.360
i have a comment and a question the first 
the comment and that is that you're using

0:35:47.360,0:35:55.920
the term spike and words matter so in a 
different sense then we use the term spike

0:35:55.920,0:36:04.000
we defined a spike as being something with uh a 
vadm higher than 160 zetta ammeter squared yeah

0:36:05.280,0:36:14.320
and that's not the way you're using it so i think 
you should use a different term peak i don't know

0:36:16.880,0:36:22.560
you know some other word because it becomes very 
confusing also if you start to use the word dip

0:36:22.560,0:36:28.000
note that dennis can't define that decrease in 
paleo intensity many years ago and so you should

0:36:28.000,0:36:32.720
look into how he defined that because we end up 
with too many words meaning too many different

0:36:32.720,0:36:34.720
things and nobody knows what 
each other's talking about

0:36:35.760,0:36:43.040
the second uh point is a question and that 
is in your bayesian model and you're allowing

0:36:43.040,0:36:48.240
um points to migrate which asian methods 
allow you to do and i think that's powerful

0:36:48.960,0:36:57.360
the my question is that do you end up with an age 
model for specimens that violates stratigraphy

0:36:58.320,0:37:08.080
because there are in some of these samples data 
sets like uh the timna 30 and the megiddo one you

0:37:08.080,0:37:13.680
know you unless people were messing around and 
taking things that are older and putting them

0:37:13.680,0:37:20.000
up in a younger strata you have a pretty good 
constraint of age ordering and i'm just curious

0:37:20.640,0:37:27.360
if your model ends up violating that 
or whether you take that into account

0:37:28.080,0:37:33.920
yeah it's a really good question uh lisa and 
it's something that we were very conscious and

0:37:33.920,0:37:40.240
careful of to retain the stratigraphy of the of 
the samples because we're we're well aware that

0:37:40.240,0:37:47.440
this is a really robust um finding from from 
from the data and so in all all these models

0:37:48.320,0:37:56.240
wherever there are um age ordering uh these are 
um these are taken into account by the modeling

0:37:57.280,0:38:05.760
okay so so when i say yeah you mentioned that the 
the ages were perturbed they but only if they are

0:38:06.320,0:38:12.640
consistent with the struct with the stratification 
constraints oh okay all right thank you

0:38:14.800,0:38:20.080
excellent thanks very much lisa and does 
anybody else have a question to throw phil's way

0:38:24.480,0:38:28.960
nobody's going to drop the hands up i can 
throw i'll let brenda brendan uh jump in

0:38:28.960,0:38:35.120
with his question first yeah so i guess uh 
this is this is a very technical question

0:38:35.920,0:38:44.400
but um for your points like in your sort of 
third and fourth spike around that area where

0:38:44.400,0:38:50.240
you might have like a bimodal distribution 
that you end up with for the ages yeah how

0:38:50.240,0:38:55.840
how does like what does your sample of convergence 
look like for that so how well does your how

0:38:55.840,0:39:01.200
old is your sample converge for things where 
you've got like a bimodal distribution of ages

0:39:02.160,0:39:10.560
um so in in you you're right to uh to raise 
the question of model convergence so is it

0:39:11.120,0:39:15.920
this is because it's all based on a numerical link 
sampling numerical sampling strategy you have to

0:39:15.920,0:39:22.560
make sure you take enough samples that your your 
distribution and any statistics derive from that

0:39:22.560,0:39:29.760
settle down and we're very careful to do that we 
didn't look specifically at these bimodal times

0:39:30.480,0:39:39.920
but but we we basically ran the model for a very 
large number of of iterations into the hundreds

0:39:39.920,0:39:45.360
of millions in order to to be absolutely 
certain that everything was well converged

0:39:49.120,0:39:49.620
okay

0:39:54.000,0:39:58.960
thanks very much brandon does anybody 
else um have questions for phil

0:40:03.280,0:40:10.240
uh cathy you've got your hand up again yeah 
i i i since there's a dose of questions maybe

0:40:10.240,0:40:18.800
a chance to ask his but i want to know what you 
think is going on yet two spikes within 30 years

0:40:18.800,0:40:26.400
of each other at 970 and 940 bc yeah yeah i kind 
of i kind of left myself open for that cathy uh

0:40:27.520,0:40:33.200
um yeah as i said here this raises 
challenging questions for core modelers

0:40:34.080,0:40:41.920
so it was hard enough to explain two spikes let 
alone six um what is what is going on well if

0:40:41.920,0:40:51.840
if this is really what happened then in order 
to get such a such a a quasi-periodic signal

0:40:53.760,0:40:58.800
the only things that i can think of that 
might explain this is some kind of wave

0:41:00.800,0:41:04.240
wave propagation which has peaks and troughs

0:41:06.960,0:41:11.360
the other way of getting it would be to have 
some some form of magnetic structure which is

0:41:12.320,0:41:18.800
sequentially or maybe a different sign so as 
it emerges from the core you get an enhancement

0:41:18.800,0:41:22.080
and then a reduction in the intensity 
followed by a further enhancement and

0:41:22.800,0:41:29.760
a reduction either way it's it's challenging to 
think of how how this is uh how this how this

0:41:29.760,0:41:34.880
happened physically um so i don't really have any 
good answers kathy it's a great question though

0:41:35.600,0:41:41.520
okay thank you thanks cathy so that sounds 
like there's some sort of mexican wave in

0:41:41.520,0:41:48.160
the core that's giving us this bumpy behavior so 
i guess actually kind of i'm just gonna jump jump

0:41:48.160,0:41:52.960
in before anybody else has a question i guess 
that kind of leads nicely to what i'm thinking

0:41:53.520,0:41:59.760
in terms of of the detailed analyses that 
you've done you know what what would you tell

0:42:01.520,0:42:07.920
the experimentalists the archaeomagnetists 
in terms of of what you would like to see

0:42:09.280,0:42:15.920
from the data what time periods what um 
information um should we be aiming for in an ideal

0:42:15.920,0:42:24.320
world assuming we can quite find those perfect 
samples yeah well the uh the levantine region is

0:42:25.520,0:42:32.800
is amazingly well sampled uh so we've got yeah 
taken at the fragment level at least there's loads

0:42:32.800,0:42:40.160
of loads of data which is fantastic a fantastic 
resource i guess what we're missing here is

0:42:40.800,0:42:49.520
a comparison to data which are averaged over a 
number of fragments so that one can be absolutely

0:42:49.520,0:42:58.320
certain of the values of intensity and also the 
the error you that are attached um to them um

0:43:01.440,0:43:06.800
i guess that's the only thing like i can 
say i i i know i know from the discussions

0:43:06.800,0:43:11.840
i've had with the various people who have 
who've analyzed this data that that um

0:43:13.280,0:43:17.440
you're all very you know there's a lot of 
certainty and a lot of confidence in in these

0:43:17.440,0:43:23.040
data points and i'm not i'm not questioning that 
at all um i'm just trying to think of ways that

0:43:24.240,0:43:30.560
the conclusions derived from the fragment 
level um data set might be also supported uh

0:43:30.560,0:43:34.560
by by um other data sets for 
example by this average data set

0:43:37.120,0:43:40.800
thanks very much um we've 
got two two more questions

0:43:40.800,0:43:43.120
and then we'll we'll draw it to a close so lisa

0:43:47.600,0:43:48.480
you're still on mute

0:43:52.640,0:44:01.520
so i was complimenting you um uh but um my 
question is now um it seems to me that since each

0:44:02.480,0:44:10.240
our the at least the the group ron schar 
eras and and i have our philosophy of

0:44:10.240,0:44:16.320
how to deal with the data has been changing 
through time and we're trying to get better

0:44:17.680,0:44:25.360
and uh and there may be uh better ways to do it 
now i think uh some of you would have heard of

0:44:25.360,0:44:33.760
uh brendan siege's um bayesian approach to to 
coming up with what's the best what's the best

0:44:33.760,0:44:41.680
intensity for a particular fragment for example 
but then to propagate those uncertainties that you

0:44:41.680,0:44:53.440
get or those uh credible intervals or um from a 
consistent um better analysis all the way up into

0:44:53.440,0:45:00.320
the model that would be my vision of how we can 
move forward so that because you have data from

0:45:00.320,0:45:08.720
many different labs different approaches different 
methods and uh and so each one of these does have

0:45:10.000,0:45:14.560
there's different uncertainties built into 
it and it seems to me i've been trying very

0:45:14.560,0:45:20.480
hard to get all of the data from our group into 
the magic database with the measurement level

0:45:21.040,0:45:31.840
data and so that it could be treated in an in 
an agnostic way and a consistent way and then

0:45:31.840,0:45:36.320
we would start from the measurements of 
the actual fragments and then move up and

0:45:36.320,0:45:44.160
propagate the uncertainties up into your model 
and proceed like that and it seems to me that um

0:45:45.680,0:45:55.440
that would overcome a lot of of uh argument about 
you know is that particular data point any good or

0:45:55.440,0:46:01.200
not or was it curved or was it blah blah blah you 
know all those arguments that we have every day

0:46:04.160,0:46:10.960
and it might be a way forward i'm just thinking 
but we don't have some people haven't put their

0:46:10.960,0:46:17.440
data into the magic database yeah or but we 
don't have access to the measurement level data

0:46:18.240,0:46:21.680
your buddy gale maybe eve 
gallery could be convinced to put

0:46:21.680,0:46:27.840
it into some way and it could be 
treated in a consistent way you know

0:46:29.280,0:46:34.960
um you you get what i mean right i do lisa 
yeah i think it's a very good idea yeah

0:46:35.840,0:46:45.760
yeah so if don't yeah you can put your data into 
the nash collection that would be great yeah yeah

0:46:46.480,0:46:56.160
it's not so easy i i tried no i know i know if 
you need help we can help thank you appreciate

0:46:56.160,0:47:01.600
it in the blood that's where i actually said so 
we'll move on uh is your question a quick one

0:47:03.920,0:47:10.080
yes it's quite quick we'll go for last one last 
quick question and then we'll wrap it up thank you

0:47:10.080,0:47:16.000
so much for this nice presentation in this very 
nice discussion i i am very surprised by the fact

0:47:16.000,0:47:23.760
that spikes disappear when we we deal with mean 
side mean values i was wondering if you have an

0:47:23.760,0:47:32.560
explanation on that and uh i am also wondering 
if are they are taking into account the single

0:47:32.560,0:47:41.920
fragment that if it really shows and correspond 
to a real spike fast variation or it could be also

0:47:42.560,0:47:50.640
an um not real effect but like let's say uh 
something that gets introduced by the error on

0:47:51.600,0:47:56.880
on the samples or maybe possible errors 
of archaeologists or collections i'm not

0:47:56.880,0:48:03.200
referring only to the levantine collection that 
is very well studied but in general and the last

0:48:03.200,0:48:09.600
point that i was wondering also when dealing with 
such data and many times we take data from them

0:48:09.600,0:48:15.840
like geometry data said that we're actually 
we have the mean values the site mini values

0:48:15.840,0:48:23.280
what do you suggest it's very close to the 
breaks uh suggesting how can we plot the fragment

0:48:23.280,0:48:30.400
values when all other data to to use to 
compare with our side beams thank you

0:48:32.800,0:48:39.280
um well i can certainly answer the first of 
your your questions uh so you yeah you raise

0:48:39.280,0:48:48.320
an interesting point here about the data set uh 
derived from group level averages of fragments uh

0:48:48.320,0:48:54.240
and so yes there is indeed only one spike 
detected in the in in the top figure here

0:48:54.240,0:49:00.000
and you can see it it largely hinges upon one 
single data point which is which is from turkey

0:49:00.720,0:49:08.240
and the reason why is a spike there i think uh 
is because the error bars are quite tight um

0:49:11.760,0:49:18.880
the reason why there aren't other spikes is 
simply because the uh the data don't require

0:49:19.920,0:49:27.360
the uh the other the other five um spikes mainly 
because there aren't that many data points

0:49:28.320,0:49:33.760
um in the fragment level data set there are 139 
uh data points here there are only 20 or 30.

0:49:34.720,0:49:45.200
uh and so really this what what what what you're 
seeing here this rather simple change is really

0:49:46.080,0:49:52.480
a manifestation of the lack of data that we have 
that can that can really uh describe rapid change

0:49:53.280,0:50:00.080
uh and as as more data points are added to this 
collection uh the more spikes may become apparent

0:50:00.080,0:50:04.720
but at the moment this is the simplest model that 
fits all the data and so the data don't require

0:50:04.720,0:50:12.160
anything more complicated so you suggest 
to use fragment levels to plot out it well

0:50:12.880,0:50:16.720
there's a choice i mean you can either choose 
a fragment level data and then you get more

0:50:16.720,0:50:22.320
obviously then you get loads of data which is 
great and then uh and that that then shows that

0:50:22.320,0:50:29.200
you need six spikes or you go with the group 
level data set uh and you and you just can

0:50:29.200,0:50:33.520
see that you just don't have that many data 
and you can fit a simple model through them

0:50:36.400,0:50:42.320
excellent thanks very much i'm going to cut it 
short there and we'll wrap up um today's seminar

0:50:42.320,0:50:53.840
thank you again to phil um for a uh another 
fantastic presentation and just let me to

0:50:54.880,0:51:01.120
um so yeah thanks again phil it's a fantastic 
presentation and another um really good um turnout

0:51:01.120,0:51:07.280
this week um just a little bit of housekeeping 
before we uh before we go um we're going to

0:51:07.280,0:51:13.120
take another short break this is this is a kind 
of a a one-off um presentation in between some

0:51:13.120,0:51:20.080
uh conferences we just had egu we'll have a 
short break for the uh irm conference in santa fe

0:51:20.720,0:51:27.520
um and we'll be returning for the next magnets 
on uh june 16th which will be given by uh andy

0:51:27.520,0:51:33.280
roberts from australia uh national university so 
at this point we will actually be switching the

0:51:33.280,0:51:39.200
time um of the seminars so we're switching to a 
time zone that's a little bit more friendly for

0:51:39.760,0:51:46.480
on the europe and eastern hemisphere region and so 
our presentations are going to be um either 8 or 9

0:51:46.480,0:51:52.320
a.m um uk time and it's going to be a little bit 
of flexibility to accommodate whether our speaker

0:51:52.320,0:51:58.880
is uh in in europe or whether they're in uh 
eastern hemisphere i've got a couple more speakers

0:51:58.880,0:52:05.840
um lined up for for and seminars afterwards and 
we'll confirm those uh details a little bit later

0:52:06.400,0:52:14.080
but we are um looking for more presenters um 
in particular we really want to encourage um

0:52:14.080,0:52:20.000
early career scientists uh early career 
researchers to to use magnets as a chance

0:52:20.000,0:52:25.200
to uh present some of their work because 
right now it's it's not easy to go and visit

0:52:25.200,0:52:32.480
um all the big fancy conferences and magnets 
is i think a really good friendly atmosphere um

0:52:32.480,0:52:39.440
for uh ecrs to to present their new research uh 
and so and it's one of the reasons actually why

0:52:40.000,0:52:46.240
um we we set up this seminar series was to get 
early career scientists recognized so that people

0:52:46.880,0:52:52.640
know who they are so please encourage um 
any of your ecr or yourself if you are

0:52:52.640,0:52:58.560
an ecr here who's thinking about giving a 
presentation so just reach out to to myself

0:52:58.560,0:53:07.360
uh or to anita and yeah just thank you everybody 
for um joining this seminar thank you very much
